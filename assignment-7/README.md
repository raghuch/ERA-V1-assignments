This folder hosts the solution for assignment 7 of the ERA-v1 course.Please click the first link of each paragraph to go to the corresponding notebook.

Steps in obtaining 99.4% test accuracy:

[Step 1](https://github.com/raghuch/ERA-V1-assignments/blob/main/assignment-7/run1.ipynb) is a naive code taken directly from the class, just to see how well the CNN performs, without paying attention to the model size. We just take the MNIST data and convert to Tensor and normalize it and added random rotation from -7 degrees to +7 degrees. It has close to 13,808 model params which are 1.73X our ideal size, but we get a max test accuracy of 99.44 in 13th epoch (but dips down by 15th epoch). The basic backbone is input block -> 1 conv blocks (conv + ReLU) -> transition block (2x2 maxpool) -> 4 conv blocks (conv + ReLU) -> GAP layer -> final conv2d layer. We set the LR to 0.01 with SGD, and a StepLR scheduler with step size 6 and gamma 0.5.

[Step 2](https://github.com/raghuch/ERA-V1-assignments/blob/main/assignment-7/run2.ipynb) again re-uses a notebook shared in the class; now trying to get the model size down first. The change is in the backbone CNN which goes like this: input block -> 2 conv blocks (conv + ReLU) -> transition block (2x2 maxpool) -> 4 conv blocks (conv + ReLU) -> final conv2d layer. The difference from Step 1 is the number of channels: Step1 has a channel progression like 1 -> 32 -> 64 -> 128 -> 32 -> 64 -> 128 -> 10 -> 10, where as we don't cross 16 channels in any conv2d layer here.  The new channel progression is 1 -> 8 -> 16 -> 10 -> 16 -> 16 -> 10 with a GAP of kernel size 6 before the last layer. CHange the lr scheduler with lr = 0.05, step_size = 6 and gamma = 0.25. The final change is the removal of one conv block just before the GAP layer, as an experiment to reduce model size, and the number of params is 7,736 (within our range). I selected that particular conv block since the number of input and output channels are the same, so I thought I could remove since number of input and output features is the same. (May work for smaller datasets like MNIST, but may fail on larger datasets since many more features are present in larger datasets). The test accuracy for 15 epochs: 99.44% but is fluctuating. While we do achieve the required test accuracy, it is not consistent, so may be we are reducing the lr too fast (_i.e._ we should increase step_size or decrease gamma or increase lr or a combination of these three).

[Step 3](https://github.com/raghuch/ERA-V1-assignments/blob/main/assignment-7/run3.ipynb): We added some augmentations in  addition to random rotation of 7degrees, _i.e._ ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1) following the example.  I have tweaked the LR and the scheduler. I changed the LR to 0.05 and the shceduler with step size 8 and gamma 0.2 i.e. every 8 steps the LR is multipled the gamma (0.2). This follows the thought in step-2 that we should increase step size or decrease gamma (or both) to get a more stable train/test error rates. If we check the training logs, we see that the test accuracy is consistently above 99% and for the last 3 epochs it is consistently above 99.4% (99.43, 99.47, 99.45)
